{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('task2_lemmas_train')\n",
    "lines = []\n",
    "for line in f:\n",
    "    lines.append(line[:-2].decode('utf-8'))\n",
    "    \n",
    "train_data = []\n",
    "for line in lines[1:]:\n",
    "    array = line.split(',')\n",
    "    train_data.append([array[1], zip(map(lambda x: x[:-2], array[2:]), map(lambda x: x[-1], array[2:]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('task2_lemmas_test')\n",
    "test_words = []\n",
    "for line in f:\n",
    "    test_words.append(line[:-2].decode('utf-8').split(',')[-1])\n",
    "test_words = test_words[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shortest_of(strings):\n",
    "    return min(strings, key=len)\n",
    "\n",
    "def long_substr(strings):\n",
    "    substr = \"\"\n",
    "    if not strings:\n",
    "        return substr\n",
    "    reference = shortest_of(strings) \n",
    "    length = len(reference)\n",
    "    for i in xrange(length):\n",
    "        for j in xrange(i + len(substr) + 1, length + 1):\n",
    "            candidate = reference[i:j]  \n",
    "            if all(candidate in text for text in strings):\n",
    "                substr = candidate\n",
    "    return substr\n",
    "\n",
    "def prefix(word, root):\n",
    "    for i in range(len(word)-len(root) + 1):\n",
    "        if word[i:i+len(root)] == root:\n",
    "            return word[:i]\n",
    "        \n",
    "def suffix(word, root):\n",
    "    ret = prefix(word[::-1], root[::-1])\n",
    "    return ret[::-1] if ret else u''\n",
    "\n",
    "def encode(array):\n",
    "    encoded = []\n",
    "    for elem in array:\n",
    "        if elem == 'N':\n",
    "            encoded.append(0)\n",
    "        if elem == 'V':\n",
    "            encoded.append(1)\n",
    "        if elem == 'A':\n",
    "            encoded.append(2)\n",
    "    return np.array(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = [[], [], [], [], []]\n",
    "suffixes = []\n",
    "for elem in train_data:\n",
    "    strings = [elem[0]] + map(lambda x: x[0], elem[1])\n",
    "    root = long_substr(strings)\n",
    "    if root != '' and len(root) > 1:\n",
    "        if root[0] == '-':\n",
    "            root = root[1:]\n",
    "    train_dataset[0].append(elem[0])\n",
    "    train_dataset[1].append(suffix(elem[0], root))\n",
    "    train_dataset[2].append(elem[1][0][0])\n",
    "    train_dataset[3].append(suffix(elem[1][0][0], root))\n",
    "    train_dataset[4].append(elem[1][0][1])\n",
    "    for x in strings:\n",
    "        suff = suffix(x, root)\n",
    "        if suff:\n",
    "            suffixes.append(suff) \n",
    "        \n",
    "suffixes = sc.unique(suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_suff_len = map(len, train_dataset[1])\n",
    "w_length_train = []\n",
    "for word in train_dataset[0]:\n",
    "    w_length_train.append(len(word))\n",
    "w_length_train = np.array(w_length_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [u'vergognerete', u'amnistiavate', u'menomazione', u'sfaldavamo', u'sfodererei']\n",
      "1 [u'erete', u'vate', u'', u'vamo', u'erei']\n",
      "2 [u'vergognare', u'amnistiare', u'menomazione', u'sfaldare', u'sfoderare']\n",
      "3 [u'are', u're', u'', u're', u'are']\n",
      "4 [u'V', u'V', u'N', u'V', u'V']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print i, train_dataset[i][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'-chiave' u'a' u'a-estate' u'a-gol' u'a-paese' u'adonna' u'aforte' u'ai'\n",
      " u'amo' u'an' u'anno' u'ano' u'arai' u'aran' u'aranno' u'are' u'arsi'\n",
      " u'ar\\xe0' u'ar\\xf2' u'ate' u'avamo' u'avate' u'bbe' u'bber' u'bbero'\n",
      " u'bbi' u'ca' u'can' u'cano' u'ccia' u'cciamo' u'ccian' u'cciano' u'cciate'\n",
      " u'ccio' u'ce' u'cei' u'cemmo' u'cendo' u'cente' u'centi' u'cerai' u'ceran'\n",
      " u'ceranno' u'cere' u'cerebbe' u'cerebber' u'cerebbero' u'cerei' u'ceremmo'\n",
      " u'ceremo' u'cereste' u'ceresti' u'cerete' u'ceron' u'cerono' u'cer\\xe0'\n",
      " u'cer\\xf2' u'ces' u'cesse' u'cesser' u'cessero' u'cessi' u'cessimo'\n",
      " u'ceste' u'cesti' u'cete' u'ceva' u'cevamo' u'cevan' u'cevano' u'cevate'\n",
      " u'cevi' u'cevo' u'ci' u'cia' u'ciamo' u'cian' u'ciano' u'ciate' u'cio'\n",
      " u'cion' u'ciono' u'ciuta' u'ciute' u'ciuti' u'ciuto' u'co' u'con' u'cono'\n",
      " u'c\\xe8' u'de' u'der' u'dere' u'dero' u'di' u'e' u'e-estati' u'e-gol'\n",
      " u'e-spia' u'ece' u'ecer' u'ecero' u'eci' u'edere' u'edonne' u'eforti'\n",
      " u'ei' u'emmo' u'en' u'endo' u'enere' u'enire' u'entare' u'ente' u'enti'\n",
      " u'erai' u'eran' u'eranno' u'erare' u'ere' u'erebbe' u'erebber' u'erebbero'\n",
      " u'erei' u'eremmo' u'eremo' u'ereste' u'eresti' u'erete' u'ernere' u'erta'\n",
      " u'erte' u'erti' u'erto' u'er\\xe0' u'er\\xf2' u'esca' u'escan' u'esce'\n",
      " u'esco' u'escon' u'escono' u'essa' u'esse' u'esser' u'essero' u'essi'\n",
      " u'essimo' u'esso' u'este' u'esti' u'ete' u'etta' u'ette' u'ettere' u'etti'\n",
      " u'etto' u'eva' u'evamo' u'evan' u'evano' u'evate' u'evi' u'evo' u'ga'\n",
      " u'gan' u'gano' u'gere' u'gga' u'ggan' u'ggano' u'ggere' u'ggo' u'ggon'\n",
      " u'ggono' u'glia' u'glian' u'gliano' u'gliate' u'gliere' u'gliono' u'go'\n",
      " u'gon' u'gono' u'guere' u'he' u'heforti' u'herai' u'heran' u'heranno'\n",
      " u'herebbe' u'herebber' u'herebbero' u'herei' u'heremmo' u'heremo'\n",
      " u'hereste' u'heresti' u'herete' u'her\\xe0' u'her\\xf2' u'hi' u'hiamo'\n",
      " u'hiare' u'hiate' u'hiato' u'hin' u'hino' u'hio' u'i' u'i-paese' u'i-spia'\n",
      " u'i-stati' u'ia' u'iamo' u'ian' u'iano' u'iare' u'iate' u'ide' u'ider'\n",
      " u'idero' u'idi' u'ie' u'ieda' u'iede' u'iedi' u'iedo' u'iedon' u'iedono'\n",
      " u'iene' u'ieni' u'ies' u'iete' u'igere' u'igruppo' u'ii' u'imere' u'in'\n",
      " u'inare' u'ingere' u'ini' u'ino' u'io' u'ion' u'iono' u'iorare' u'iore'\n",
      " u'iori' u'ire' u'ise' u'iser' u'isero' u'isi' u'ita' u'ite' u'iti' u'ito'\n",
      " u'iuta' u'iute' u'iuti' u'iuto' u'iviri' u'le' u'lere' u'lero' u'lga'\n",
      " u'lgan' u'lgano' u'lgo' u'lgon' u'lgono' u'li' u'lser' u'lsero' u'lsi'\n",
      " u'ls\\xe8' u'lta' u'lte' u'lti' u'lto' u'mmo' u'mo' u'mpere' u'n' u'nare'\n",
      " u'nde' u'ndere' u'ndo' u'ne' u'nei' u'nemmo' u'nendo' u'nente' u'nenti'\n",
      " u'ner' u'nerai' u'nere' u'nerebber' u'nerebbero' u'nerei' u'neremmo'\n",
      " u'neremo' u'nereste' u'neresti' u'nero' u'neron' u'nerono' u'ner\\xe0'\n",
      " u'ner\\xf2' u'nes' u'nesse' u'nesser' u'nessero' u'nessi' u'nessimo'\n",
      " u'neste' u'nesti' u'nete' u'neva' u'nevamo' u'nevan' u'nevano' u'nevate'\n",
      " u'nevi' u'nevo' u'nga' u'ngan' u'ngano' u'ngo' u'ngon' u'ngono' u'ni'\n",
      " u'niamo' u'niate' u'nire' u'nno' u'no' u'nte' u'nti' u'nto' u'nute'\n",
      " u'nuti' u'n\\xe8' u'o' u'o-chiave' u'o-stato' u'ocemmo' u'ocendo' u'ocesse'\n",
      " u'oceste' u'ocesti' u'ocevamo' u'ocevano' u'ocevate' u'ocevi' u'ocevo'\n",
      " u'ociate' u'ogruppo' u'olere' u'ompere' u'on' u'onare' u'ondere' u'ono'\n",
      " u'ore' u'orire' u'ossa' u'osse' u'osser' u'ossero' u'ossi' u'osso'\n",
      " u'otere' u'otta' u'otte' u'otti' u'otto' u'oviro' u'que' u'quer' u'quero'\n",
      " u'qui' u'r' u'rai' u'ran' u'rande' u'ranno' u'rare' u're' u'rebbe'\n",
      " u'rebber' u'rebbero' u'rei' u'remmo' u'remo' u'rere' u'reste' u'resti'\n",
      " u'reta' u'rete' u'reto' u'ri' u'rice' u'rici' u'rire' u'rrai' u'rran'\n",
      " u'rranno' u'rre' u'rrebbe' u'rrebber' u'rrebbero' u'rrei' u'rremmo'\n",
      " u'rremo' u'rreste' u'rresti' u'rrete' u'rr\\xe0' u'rr\\xf2' u'rsi' u'r\\xe0'\n",
      " u'r\\xf2' u's' u'sa' u'sare' u'sca' u'scan' u'scano' u'sce' u'scere' u'sci'\n",
      " u'sco' u'scon' u'scono' u'se' u'ser' u'sero' u'si' u'so' u'ssa' u'ssan'\n",
      " u'ssare' u'sse' u'sser' u'ssero' u'ssi' u'ssiate' u'ssime' u'ssimo' u'sso'\n",
      " u'sson' u'ssono' u'ssuta' u'ssute' u'ssuti' u'ssuto' u'ss\\xe8' u'sta'\n",
      " u'stare' u'ste' u'sti' u'sto' u's\\xe8' u'ta' u'tare' u'tata' u'tate'\n",
      " u'tati' u'tato' u'te' u'tere' u'ti' u'to' u'tta' u'ttare' u'tte' u'tter'\n",
      " u'ttere' u'ttero' u'tti' u'tto' u'uocere' u'uoi' u'uoian' u'uoiano'\n",
      " u'uoio' u'uoiono' u'uotere' u'uovere' u'uppe' u'upper' u'uppero' u'uppi'\n",
      " u'urre' u'usa' u'uscire' u'use' u'user' u'usero' u'usi' u'uso' u'uta'\n",
      " u'ute' u'uti' u'uto' u'u\\xf2' u'va' u'vamo' u'van' u'vano' u'vate' u've'\n",
      " u'vente' u'venti' u'ver' u'vere' u'vero' u'vi' u'vo' u'x' u'y' u'\\xe0'\n",
      " u'\\xe8' u'\\xe8i' u'\\xec' u'\\xf2']\n"
     ]
    }
   ],
   "source": [
    "print suffixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Предсказание части речи</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4156fed96ae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'char_wb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowercase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pavel/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pavel/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    789\u001b[0m                           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m                           dtype=self.dtype)\n\u001b[0;32m--> 791\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pavel/anaconda2/lib/python2.7/site-packages/scipy/sparse/compressed.pyc\u001b[0m in \u001b[0;36msort_indices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_sorted_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m             _sparsetools.csr_sort_indices(len(self.indptr) - 1, self.indptr,\n\u001b[0;32m-> 1018\u001b[0;31m                                           self.indices, self.data)\n\u001b[0m\u001b[1;32m   1019\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_sorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 8), lowercase = True)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "print np.mean(cross_val_score(LogisticRegression(C=50), X, train_dataset[4], verbose=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "c_range = range(2, 10)\n",
    "for c in c_range:\n",
    "    cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, c), lowercase = True)\n",
    "    X = cv.fit_transform(train_dataset[0])\n",
    "    scores.append(np.mean(cross_val_score(SVC(C=50, kernel='linear', class_weight='balanced'), X, train_dataset[4], verbose=2)))\n",
    "    print i, scores[-1]\n",
    "index = np.argmax(scores)\n",
    "print c_range[index], scores[index]\n",
    "plt.plot(c_range, score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Предсказание суффикса</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavel/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 4.8min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total= 5.2min\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 5.3min\n",
      "0.94684858034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 15.4min finished\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 7), lowercase = True)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "print np.mean(cross_val_score(LogisticRegression(), X, train_dataset[3], verbose=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 6.5min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  6.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total= 7.6min\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 7.7min\n",
      "0.949908308836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 21.8min finished\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 10), lowercase = True)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "print np.mean(cross_val_score(LogisticRegression(C=50), X, train_dataset[3], verbose=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 4.8min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total= 5.5min\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 5.6min\n",
      "0.951164005539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 15.8min finished\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 7), lowercase = True)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "print np.mean(cross_val_score(LogisticRegression(C=50), X, train_dataset[3], verbose=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.93828457213\n",
      "5 0.947573101747\n",
      "6 0.950118631636\n",
      "7 0.951164005539\n",
      "8 0.951088196506\n",
      "9 0.950481355555\n",
      "7 0.951164005539\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-d39c824098a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mc_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "c_range = range(4, 10)\n",
    "for i in c_range:\n",
    "    cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, i), lowercase = True)\n",
    "    X = cv.fit_transform(train_dataset[0])\n",
    "    scores.append(np.mean(cross_val_score(LogisticRegression(C=50), X, train_dataset[3])))\n",
    "    print i, scores[-1]\n",
    "index = np.argmax(scores)\n",
    "print c_range[index], scores[index]\n",
    "plt.plot(c_range, score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Предсказание куска, который нужно отрезать</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 2.2min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total= 2.2min\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 2.6min\n",
      "0.9267869874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  7.0min finished\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 8), lowercase = True)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "print np.mean(cross_val_score(LogisticRegression(C=50), X, train_suff_len, verbose=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 4.1min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total= 3.6min\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 3.9min\n",
      "0.927705723738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 11.7min finished\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 8), lowercase = True, max_df=0.5)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "print np.mean(cross_val_score(LogisticRegression(C=50), X, train_suff_len, verbose=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 2.0min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total= 2.0min\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 1.9min\n",
      "0.928793025848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  5.9min finished\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 10), lowercase = True, max_df=0.5)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "print np.mean(cross_val_score(LogisticRegression(C=50), X, train_suff_len, verbose=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 2.2min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total= 2.2min\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 2.5min\n",
      "0.927427570278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  6.9min finished\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 8), lowercase = True, max_df=0.9)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "print np.mean(cross_val_score(LogisticRegression(C=50), X, train_suff_len, verbose=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 2.0min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total= 1.8min\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 2.0min\n",
      "0.927570863321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  5.9min finished\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 8), lowercase = True, max_df=0.5)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "print np.mean(cross_val_score(LogisticRegression(C=30), X, train_suff_len, verbose=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Полный ответ</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class predicted\n",
      "len predicted\n",
      "ending predicted\n",
      "0.914784221173\n",
      "CPU times: user 1h 12min 27s, sys: 1min 51s, total: 1h 14min 18s\n",
      "Wall time: 19min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 8), lowercase = True)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "ind = 4*X.shape[0]/5\n",
    "clf = LogisticRegression(C=50)\n",
    "clf.fit(X[:ind], train_dataset[4][:ind])\n",
    "class_pred = clf.predict(X[ind:])\n",
    "print 'class predicted'\n",
    "\n",
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 10), lowercase = True, max_df=0.5)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "dummy_features = ohe.fit_transform(encode(train_dataset[4][:ind] + class_pred.tolist()).reshape(-1, 1))\n",
    "X = csr_matrix(hstack([X, csr_matrix(dummy_features)]))\n",
    "\n",
    "clf.fit(X[:ind], train_suff_len[:ind])\n",
    "len_suff_pred = clf.predict(X[ind:])\n",
    "print 'len predicted'\n",
    "\n",
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 7), lowercase = True)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "X = csr_matrix(hstack([X, csr_matrix(train_suff_len[:ind] + len_suff_pred.tolist()).transpose()]))\n",
    "\n",
    "clf.fit(X[:ind], train_dataset[3][:ind])\n",
    "ending_pred = clf.predict(X[ind:])\n",
    "print 'ending predicted'\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(len_suff_pred)):\n",
    "    cutted_word = train_dataset[0][ind+i][:-len_suff_pred[i]] if len_suff_pred[i] > 0 else train_dataset[0][ind+i]\n",
    "    predictions.append(cutted_word + ending_pred[i] + '+' + class_pred[i])\n",
    "\n",
    "true_values = []\n",
    "for i in range(len(len_suff_pred)):\n",
    "    true_values.append(train_dataset[2][ind+i] + '+' + train_dataset[4][ind+i])\n",
    "\n",
    "print accuracy_score(true_values, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "class predicted\n",
      "len predicted\n",
      "ending predicted\n",
      "23727 0.921527309508\n",
      "2\n",
      "class predicted\n",
      "len predicted\n",
      "ending predicted\n",
      "23727 0.921105866487\n",
      "3\n",
      "class predicted\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6787fa306208>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'char_wb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowercase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mohe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdummy_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mohe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclass_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pavel/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pavel/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1, 20):\n",
    "    print i\n",
    "    cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 8), lowercase = True)\n",
    "    X = cv.fit_transform(train_dataset[0])\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    dummy_features = ohe.fit_transform(encode(train_dataset[4][:ind] + class_pred.tolist()).reshape(-1, 1))\n",
    "    X = csr_matrix(hstack([X, csr_matrix(dummy_features), csr_matrix(train_suff_len[:ind] + len_suff_pred.tolist()).transpose()]))\n",
    "    cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 5), lowercase = True)\n",
    "    X = csr_matrix(hstack([X, cv.fit_transform(train_dataset[3][:ind] + ending_pred.tolist())]))\n",
    "    \n",
    "    ind = 4*X.shape[0]/5\n",
    "    clf = LogisticRegression(C=50)\n",
    "    clf.fit(X[:ind], train_dataset[4][:ind])\n",
    "    class_pred = clf.predict(X[ind:])\n",
    "    print 'class predicted'\n",
    "\n",
    "    cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 10), lowercase = True, max_df=0.5)\n",
    "    X = cv.fit_transform(train_dataset[0])\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    dummy_features = ohe.fit_transform(encode(train_dataset[4][:ind] + class_pred.tolist()).reshape(-1, 1))\n",
    "    X = csr_matrix(hstack([X, csr_matrix(dummy_features), csr_matrix(train_suff_len[:ind] + len_suff_pred.tolist()).transpose()]))\n",
    "    cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 5), lowercase = True)\n",
    "    X = csr_matrix(hstack([X, cv.fit_transform(train_dataset[3][:ind] + ending_pred.tolist())]))\n",
    "    clf.fit(X[:ind], train_suff_len[:ind])\n",
    "    len_suff_pred = clf.predict(X[ind:])\n",
    "    print 'len predicted'\n",
    "\n",
    "    cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 7), lowercase = True)\n",
    "    X = cv.fit_transform(train_dataset[0])\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    dummy_features = ohe.fit_transform(encode(train_dataset[4][:ind] + class_pred.tolist()).reshape(-1, 1))\n",
    "    X = csr_matrix(hstack([X, csr_matrix(dummy_features), csr_matrix(train_suff_len[:ind] + len_suff_pred.tolist()).transpose()]))\n",
    "    cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 5), lowercase = True)\n",
    "    X = csr_matrix(hstack([X, cv.fit_transform(train_dataset[3][:ind] + ending_pred.tolist())]))\n",
    "\n",
    "    clf.fit(X[:ind], train_dataset[3][:ind])\n",
    "    ending_pred = clf.predict(X[ind:])\n",
    "    print 'ending predicted'\n",
    "\n",
    "    predictions = []\n",
    "    for i in range(len(len_suff_pred)):\n",
    "        cutted_word = train_dataset[0][ind+i][:-len_suff_pred[i]] if len_suff_pred[i] > 0 else train_dataset[0][ind+i]\n",
    "        predictions.append(cutted_word + ending_pred[i] + '+' + class_pred[i])\n",
    "\n",
    "    true_values = []\n",
    "    for i in range(len(len_suff_pred)):\n",
    "        true_values.append(train_dataset[2][ind+i] + '+' + train_dataset[4][ind+i])\n",
    "\n",
    "    print i, accuracy_score(true_values, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class predicted\n",
      "len predicted\n",
      "ending predicted\n",
      "0.904121712744\n",
      "CPU times: user 48min 52s, sys: 57.6 s, total: 49min 49s\n",
      "Wall time: 12min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 7), lowercase = True)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "ind = 4*X.shape[0]/5\n",
    "clf = LogisticRegression(C=50)\n",
    "clf.fit(X[:ind], train_dataset[4][:ind])\n",
    "class_pred = clf.predict(X[ind:])\n",
    "print 'class predicted'\n",
    "\n",
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 7), lowercase = True, max_df=0.5)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "dummy_features = ohe.fit_transform(encode(train_dataset[4][:ind] + class_pred.tolist()).reshape(-1, 1))\n",
    "X = csr_matrix(hstack([X, csr_matrix(dummy_features)]))\n",
    "\n",
    "clf.fit(X[:ind], train_suff_len[:ind])\n",
    "len_suff_pred = clf.predict(X[ind:])\n",
    "print 'len predicted'\n",
    "\n",
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 7), lowercase = True)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "X = csr_matrix(hstack([X, csr_matrix(train_suff_len[:ind] + len_suff_pred.tolist()).transpose()]))\n",
    "\n",
    "clf.fit(X[:ind], train_dataset[3][:ind])\n",
    "ending_pred = clf.predict(X[ind:])\n",
    "print 'ending predicted'\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(len_suff_pred)):\n",
    "    cutted_word = train_dataset[0][ind+i][:-len_suff_pred[i]] if len_suff_pred[i] > 0 else train_dataset[0][ind+i]\n",
    "    predictions.append(cutted_word + ending_pred[i] + '+' + class_pred[i])\n",
    "\n",
    "true_values = []\n",
    "for i in range(len(len_suff_pred)):\n",
    "    true_values.append(train_dataset[2][ind+i] + '+' + train_dataset[4][ind+i])\n",
    "\n",
    "print accuracy_score(true_values, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Дополнения в процессе </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class predicted\n",
      "CPU times: user 2min 53s, sys: 3.51 s, total: 2min 56s\n",
      "Wall time: 51.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 8), lowercase = True)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "ind = 4*X.shape[0]/5\n",
    "clf = LogisticRegression(C=50)\n",
    "clf.fit(X[:ind], train_dataset[4][:ind])\n",
    "class_pred = clf.predict(X[ind:])\n",
    "print 'class predicted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len predicted\n",
      "CPU times: user 10min 54s, sys: 12.5 s, total: 11min 6s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 10), lowercase = True, max_df=0.5)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "dummy_features = ohe.fit_transform(encode(train_dataset[4][:ind] + class_pred.tolist()).reshape(-1, 1))\n",
    "X = csr_matrix(hstack([X, csr_matrix(dummy_features)]))\n",
    "\n",
    "clf.fit(X[:ind], train_suff_len[:ind])\n",
    "len_suff_pred = clf.predict(X[ind:])\n",
    "print 'len predicted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ending predicted\n",
      "0.914784221173\n",
      "CPU times: user 40min 45s, sys: 1min 2s, total: 41min 47s\n",
      "Wall time: 11min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 7), lowercase = True)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "X = csr_matrix(hstack([X, csr_matrix(train_suff_len[:ind] + len_suff_pred.tolist()).transpose()]))\n",
    "\n",
    "clf.fit(X[:ind], train_dataset[3][:ind])\n",
    "ending_pred = clf.predict(X[ind:])\n",
    "print 'ending predicted'\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(len_suff_pred)):\n",
    "    cutted_word = train_dataset[0][ind+i][:-len_suff_pred[i]] if len_suff_pred[i] > 0 else train_dataset[0][ind+i]\n",
    "    predictions.append(cutted_word + ending_pred[i] + '+' + class_pred[i])\n",
    "\n",
    "true_values = []\n",
    "for i in range(len(len_suff_pred)):\n",
    "    true_values.append(train_dataset[2][ind+i] + '+' + train_dataset[4][ind+i])\n",
    "    \n",
    "print accuracy_score(true_values, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.9185 c С=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.912213418746\n"
     ]
    }
   ],
   "source": [
    "print accuracy_score(true_values, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print range(10)[np.arrange()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
