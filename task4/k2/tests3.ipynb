{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('task2_lemmas_train')\n",
    "lines = []\n",
    "for line in f:\n",
    "    lines.append(line[:-2].decode('utf-8'))\n",
    "    \n",
    "train_data = []\n",
    "for line in lines[1:]:\n",
    "    array = line.split(',')\n",
    "    train_data.append([array[1], zip(map(lambda x: x[:-2], array[2:]), map(lambda x: x[-1], array[2:]))])\n",
    "    \n",
    "f = open('task2_lemmas_test')\n",
    "test_words = []\n",
    "for line in f:\n",
    "    test_words.append(line[:-2].decode('utf-8').split(',')[-1])\n",
    "test_words = test_words[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shortest_of(strings):\n",
    "    return min(strings, key=len)\n",
    "\n",
    "def long_substr(strings):\n",
    "    substr = \"\"\n",
    "    if not strings:\n",
    "        return substr\n",
    "    reference = shortest_of(strings) \n",
    "    length = len(reference)\n",
    "    for i in xrange(length):\n",
    "        for j in xrange(i + len(substr) + 1, length + 1):\n",
    "            candidate = reference[i:j]  \n",
    "            if all(candidate in text for text in strings):\n",
    "                substr = candidate\n",
    "    return substr\n",
    "\n",
    "def prefix(word, root):\n",
    "    for i in range(len(word)-len(root) + 1):\n",
    "        if word[i:i+len(root)] == root:\n",
    "            return word[:i]\n",
    "        \n",
    "def suffix(word, root):\n",
    "    ret = prefix(word[::-1], root[::-1])\n",
    "    return ret[::-1] if ret else u''\n",
    "\n",
    "def encode(array):\n",
    "    encoded = []\n",
    "    for elem in array:\n",
    "        if elem == 'N':\n",
    "            encoded.append(0)\n",
    "        if elem == 'V':\n",
    "            encoded.append(1)\n",
    "        if elem == 'A':\n",
    "            encoded.append(2)\n",
    "    return np.array(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = [[], [], [], [], []]\n",
    "suffixes = []\n",
    "for elem in train_data:\n",
    "    strings = [elem[0]] + map(lambda x: x[0], elem[1])\n",
    "    root = long_substr(strings)\n",
    "    if root != '' and len(root) > 1:\n",
    "        if root[0] == '-':\n",
    "            root = root[1:]\n",
    "    train_dataset[0].append(elem[0])\n",
    "    train_dataset[1].append(suffix(elem[0], root))\n",
    "    train_dataset[2].append(elem[1][0][0])\n",
    "    train_dataset[3].append(suffix(elem[1][0][0], root))\n",
    "    train_dataset[4].append(elem[1][0][1])\n",
    "    for x in strings:\n",
    "        suff = suffix(x, root)\n",
    "        if suff:\n",
    "            suffixes.append(suff) \n",
    "        \n",
    "suffixes = sc.unique(suffixes)\n",
    "\n",
    "train_suff_len = map(len, train_dataset[1])\n",
    "w_length_train = []\n",
    "for word in train_dataset[0]:\n",
    "    w_length_train.append(len(word))\n",
    "w_length_train = np.array(w_length_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Stack </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class predicted\n",
      "len predicted\n",
      "ending predicted\n",
      "0.904585300067\n",
      "CPU times: user 46min 2s, sys: 1min 8s, total: 47min 10s\n",
      "Wall time: 12min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 8), lowercase = True)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "ind = 4*X.shape[0]/5\n",
    "ind1 = 4*ind/5\n",
    "clf = LogisticRegression(C=50)\n",
    "clf.fit(X[:ind1], train_dataset[4][:ind1])\n",
    "class_pred = clf.predict(X[ind:])\n",
    "print 'class predicted'\n",
    "\n",
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 10), lowercase = True, max_df=0.5)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "dummy_features = ohe.fit_transform(encode(train_dataset[4][:ind] + class_pred.tolist()).reshape(-1, 1))\n",
    "X = csr_matrix(hstack([X, csr_matrix(dummy_features)]))\n",
    "\n",
    "clf.fit(X[:ind1], train_suff_len[:ind1])\n",
    "len_suff_pred = clf.predict(X[ind:])\n",
    "print 'len predicted'\n",
    "\n",
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 7), lowercase = True)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "X = csr_matrix(hstack([X, csr_matrix(train_suff_len[:ind] + len_suff_pred.tolist()).transpose()]))\n",
    "\n",
    "clf.fit(X[:ind1], train_dataset[3][:ind1])\n",
    "ending_pred = clf.predict(X[ind:])\n",
    "print 'ending predicted'\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(len_suff_pred)):\n",
    "    cutted_word = train_dataset[0][ind+i][:-len_suff_pred[i]] if len_suff_pred[i] > 0 else train_dataset[0][ind+i]\n",
    "    predictions.append(cutted_word + ending_pred[i] + '+' + class_pred[i])\n",
    "\n",
    "true_values = []\n",
    "for i in range(len(len_suff_pred)):\n",
    "    true_values.append(train_dataset[2][ind+i] + '+' + train_dataset[4][ind+i])\n",
    "\n",
    "print accuracy_score(true_values, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class predicted\n",
      "len predicted\n",
      "ending predicted\n",
      "0.882880984491\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 8), lowercase = True)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "dummy_features = ohe.fit_transform(encode(train_dataset[4][:ind] + class_pred.tolist()).reshape(-1, 1))\n",
    "X = csr_matrix(hstack([X, csr_matrix(dummy_features), csr_matrix(train_suff_len[:ind] + len_suff_pred.tolist()).transpose()]))\n",
    "#dummy_features = ohe.fit_transform(encode(train_dataset[3][:ind] + ending_pred.tolist()).reshape(-1, 1))\n",
    "#X = csr_matrix(hstack(pX, csr_matrix(dummy_features)))\n",
    "clf = LogisticRegression(C=50)\n",
    "clf.fit(X[ind1:ind], train_dataset[4][ind1:ind])\n",
    "class_pred = clf.predict(X[ind:])\n",
    "print 'class predicted'\n",
    "\n",
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 10), lowercase = True, max_df=0.5)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "dummy_features = ohe.fit_transform(encode(train_dataset[4][:ind] + class_pred.tolist()).reshape(-1, 1))\n",
    "X = csr_matrix(hstack([X, csr_matrix(dummy_features), csr_matrix(train_suff_len[:ind] + len_suff_pred.tolist()).transpose()]))\n",
    "#dummy_features = ohe.fit_transform(encode(train_dataset[3][:ind] + ending_pred.tolist()).reshape(-1, 1))\n",
    "#X = csr_matrix(hstack(pX, csr_matrix(dummy_features)))\n",
    "clf.fit(X[ind1:ind], train_suff_len[ind1:ind])\n",
    "len_suff_pred = clf.predict(X[ind:])\n",
    "print 'len predicted'\n",
    "\n",
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(1, 7), lowercase = True)\n",
    "X = cv.fit_transform(train_dataset[0])\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "dummy_features = ohe.fit_transform(encode(train_dataset[4][:ind] + class_pred.tolist()).reshape(-1, 1))\n",
    "X = csr_matrix(hstack([X, csr_matrix(dummy_features), csr_matrix(train_suff_len[:ind] + len_suff_pred.tolist()).transpose()]))\n",
    "#dummy_features = ohe.fit_transform(encode(train_dataset[3][:ind] + ending_pred.tolist()).reshape(-1, 1))\n",
    "#X = csr_matrix(hstack(pX, csr_matrix(dummy_features)))\n",
    "\n",
    "clf.fit(X[ind1:ind], train_dataset[3][ind1:ind])\n",
    "ending_pred = clf.predict(X[ind:])\n",
    "print 'ending predicted'\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(len_suff_pred)):\n",
    "    cutted_word = train_dataset[0][ind+i][:-len_suff_pred[i]] if len_suff_pred[i] > 0 else train_dataset[0][ind+i]\n",
    "    predictions.append(cutted_word + ending_pred[i] + '+' + class_pred[i])\n",
    "\n",
    "true_values = []\n",
    "for i in range(len(len_suff_pred)):\n",
    "    true_values.append(train_dataset[2][ind+i] + '+' + train_dataset[4][ind+i])\n",
    "\n",
    "print accuracy_score(true_values, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "sequence too large; cannot be greater than 32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-204844e70322>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mohe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdummy_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mohe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchararray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mending_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/pavel/anaconda2/lib/python2.7/site-packages/numpy/core/defchararray.pyc\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(subtype, shape, itemsize, unicode, buffer, offset, strides, order)\u001b[0m\n\u001b[1;32m   1833\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m             self = ndarray.__new__(subtype, shape, (dtype, itemsize),\n\u001b[0;32m-> 1835\u001b[0;31m                                    order=order)\n\u001b[0m\u001b[1;32m   1836\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m             self = ndarray.__new__(subtype, shape, (dtype, itemsize),\n",
      "\u001b[0;31mValueError\u001b[0m: sequence too large; cannot be greater than 32"
     ]
    }
   ],
   "source": [
    "ohe = OneHotEncoder(sparse=False)\n",
    "dummy_features = ohe.fit_transform(np.chararray(train_dataset[3][:ind] + ending_pred.tolist()).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
