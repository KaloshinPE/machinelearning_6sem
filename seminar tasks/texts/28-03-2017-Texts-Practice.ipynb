{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-03-28 13:07:36--  https://raw.githubusercontent.com/ml-mipt/ml-mipt-part1/master/2017/seminars/493/dataset1.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.36.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.36.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 448467 (438K) [text/plain]\n",
      "Saving to: ‘dataset1.tsv’\n",
      "\n",
      "100%[======================================>] 448 467      750KB/s   in 0,6s   \n",
      "\n",
      "2017-03-28 13:07:37 (750 KB/s) - ‘dataset1.tsv’ saved [448467/448467]\n",
      "\n",
      "--2017-03-28 13:07:37--  https://raw.githubusercontent.com/ml-mipt/ml-mipt-part1/master/2017/seminars/493/dataset2.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.36.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.36.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3371540 (3,2M) [text/plain]\n",
      "Saving to: ‘dataset2.tsv’\n",
      "\n",
      "100%[======================================>] 3 371 540   3,03MB/s   in 1,1s   \n",
      "\n",
      "2017-03-28 13:07:39 (3,03 MB/s) - ‘dataset2.tsv’ saved [3371540/3371540]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ml-mipt/ml-mipt-part1/master/2017/seminars/493/dataset1.tsv\n",
    "!wget https://raw.githubusercontent.com/ml-mipt/ml-mipt-part1/master/2017/seminars/493/dataset2.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset1.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    only_alphabetical = re.sub(\"[^a-zA-Z0-9 ]+\", \"\", text)\n",
    "    with_folded_whitespaces = re.sub(\"\\s+\", \" \", only_alphabetical)\n",
    "    lowered_text = with_folded_whitespaces.lower()\n",
    "    \n",
    "    return lowered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert clean_text(\"THE DA VINCI CODE is AN AWESOME BOOK\") == \"the da vinci code is an awesome book\"\n",
    "assert clean_text(\"ASDASD131231gs...as,da,s,,313-----sad  asdadasa\") == \"asdasd131231gsasdas313sad asdadasa\"\n",
    "assert not clean_text(\".....___----****&&&^^\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler ive ever read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the da vinci code a lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the da vinci code a lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the da vinci code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>thats not even an exaggeration and at midnight...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   positive                                               text\n",
       "0         1  this was the first clive cussler ive ever read...\n",
       "1         1                    i liked the da vinci code a lot\n",
       "2         1                    i liked the da vinci code a lot\n",
       "3         1  i liked the da vinci code but it ultimatly did...\n",
       "4         1  thats not even an exaggeration and at midnight..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"] = df.text.apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/home/eltyshev/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/home/eltyshev/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from nltk)\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'i like the da vinci code a lot'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "st = PorterStemmer()\n",
    "def stem_text(text):\n",
    "    words = text.split()\n",
    "    words = [st.stem(word) for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "stem_text(\"i liked the da vinci code a lot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert stem_text(\"i liked the da vinci code a lot\") == \"i like the da vinci code a lot\"\n",
    "assert (stem_text(\"this prevents urllib3 from configuring ssl appropriately and may cause certain ssl connections to fail\") ==\n",
    "        \"thi prevent urllib3 from configur ssl appropri and may caus certain ssl connect to fail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words before stemming: 2325\n",
      "Unique words after stemming: 2037\n"
     ]
    }
   ],
   "source": [
    "print \"Unique words before stemming:\", np.unique(\" \".join(df.text.values).split()).shape[0]\n",
    "df.text = df.text.apply(stem_text)\n",
    "print \"Unique words after stemming:\", np.unique(\" \".join(df.text.values).split()).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = df.text\n",
    "labels = df.positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer, FeatureHasher\n",
    "\n",
    "#transformer = CountVectorizer()\n",
    "#matrix = transformer.fit_transform(texts)\n",
    "transformer = FeatureHasher(input_type=\"string\")\n",
    "matrix = transformer.fit_transform(map(lambda x: x.split(), texts)) # в случае FeatureHasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947077164855\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=5.2)\n",
    "print cross_val_score(clf, matrix, labels, scoring=\"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_range = np.arange(0, 10, 0.1)[1:]\n",
    "scores = []\n",
    "for c in c_range:\n",
    "    scores.append(cross_val_score(LogisticRegression(C=c), matrix, labels, scoring=\"accuracy\").mean())\n",
    "\n",
    "index = np.argmax(scores)\n",
    "print c_range[index], scores[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
